{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Masked Autoregressive Flow\n",
    "Here we go through a simple example of recovering a posterior with SBI. For this, we will use an analytic model, for which we can then compare the result of a fitted and analytic likelihood (and posterior).\n",
    " \n",
    "## Model\n",
    "The simple model has two parameteres, $\\mu_0$ and $\\sigma_0$. Prior distributions are defined as:\n",
    "$$P(\\mu_0) = \\frac{1}{2\\pi} \\exp \\left(-\\frac{1}{2} \\mu_0^2\\right)$$\n",
    "$$P(\\log_{10} \\sigma_0) = \\begin{cases} 1 & \\text{for} -1/2 < \\log_{10} \\sigma_0 < 1/2 \\\\ 0 & \\text{otherwise}\\end{cases}$$\n",
    "\n",
    "We'll work in a 5D data space, where the analytic likelihood is given by:\n",
    "$$P(\\boldsymbol{d} | \\mu_0, \\sigma_0) = \\frac{1}{(2\\pi)^{5/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\boldsymbol{d} - \\boldsymbol{\\mu})^T \\Sigma^{-1} (\\boldsymbol{d} - \\boldsymbol{\\mu})\\right) \\, ,$$\n",
    "where $$\\boldsymbol{\\mu} = (1^2, 2^2, 3^2, 4^2, 5^2) \\cdot \\mu_0 \\, ,$$\n",
    "$$\\Sigma = \n",
    "\\begin{pmatrix}\n",
    "1^2 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 2^2 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 3^2 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 4^2 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 5^2\n",
    "\\end{pmatrix} \\cdot \\sigma^2_0 \\, .$$\n",
    "\n",
    "## Fitting the likelihood\n",
    "To fit the likelihood we create 100 000 data samples of the likelihood function, for $\\mu_0$ and $\\sigma_0$ pulled from the prior. In this way, the training set follows the prior distribution. This is not strictly necessary. Because we fit the likelihood, final posterior recovery will be insensitive on how were the training samples constructed. The benefit of using the real prior however, is to allow the network to learn important part of the parameter space.\n",
    "\n",
    "## Mock measurement\n",
    "With this at hand, we sample 10 points from the analytic likelihood as our mock measurement, for fixed $\\mu_0 = 1$, $\\sigma_0 = 1$ and try to recover posterior using the fitted likelihood. The final result is then compared with the posterior recovered from the analytic likelihood.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from functools import partial\n",
    "import logging\n",
    "import corner\n",
    "import ultranest\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from py21cmlikelihoods import ConditionalMaskedAutoregressiveFlow\n",
    "from py21cmlikelihoods.utils import prepare_dataset\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "ultranest_logger = logging.getLogger(\"ultranest\")\n",
    "ultranest_logger.addHandler(logging.NullHandler())\n",
    "ultranest_logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# constructing the likelihood\n",
    "NDE = ConditionalMaskedAutoregressiveFlow(\n",
    "    cond_n_dim = 2, \n",
    "    n_dim = 5, \n",
    "    hidden_units_dim = 50,\n",
    "    n_MADEs = 3,\n",
    "    optimizer = tf.optimizers.Adam(1e-3), \n",
    "    kernel_initializer=tf.keras.initializers.RandomNormal,\n",
    "    kernel_initializer_kwargs = {\"mean\":0.0, \"stddev\": 1e-4, \"seed\":None},\n",
    "    # kernel_initializer = \"glorot_uniform\",\n",
    "    # kernel_initializer_kwargs = {},\n",
    "    bias_initializer = \"zeros\",\n",
    "    bias_initializer_kwargs={},\n",
    "    last_layer_bias_initializer = \"zeros\",\n",
    "    kernel_regularizer = tf.keras.regularizers.L1L2(l1=1e-6, l2=1e-6),\n",
    "    bias_regularizer = tf.keras.regularizers.L1L2(l1=1e-6, l2=1e-6),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# constructing the training set\n",
    "mu_0 = np.random.normal(0.0, 1.0, size = 100000)\n",
    "sigma_0 = np.random.uniform(0.5, 5.0, size = 100000)\n",
    "params = np.stack([mu_0, sigma_0], axis = -1)\n",
    "data = np.array([np.random.multivariate_normal(np.arange(1, 6)**2 * m, np.diag(np.arange(1, 6)**2 * s**2)) for m, s in params])\n",
    "\n",
    "training_set = prepare_dataset(NDE, data_samples=data, param_samples=params, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training the likelihood\n",
    "NDE.train(\n",
    "    epochs = 5,\n",
    "    dataset = training_set,\n",
    "    save = False,\n",
    "    save_history = False,\n",
    "    verbose = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing the likelihood shape for mu_0 = 2, sigma_0 = 1\n",
    "NDE_sample = NDE.sample(100000, conditional = tf.constant([2.0, 1.0])).numpy()\n",
    "analytic_sample = np.random.multivariate_normal(np.arange(1, 6)**2 * 2, np.diag(np.arange(1, 6)**2), 100000)\n",
    "\n",
    "fig = plt.figure(figsize = (8, 8))\n",
    "corner.corner(\n",
    "    NDE_sample, \n",
    "    fig = fig, \n",
    "    color = \"blue\", \n",
    "    truths = np.arange(1, 6)**2 * 2, \n",
    "    levels = (0.68, 0.95), \n",
    "    plot_contour=True,\n",
    "    plot_density=False,\n",
    "    plot_datapoints=False,\n",
    ")\n",
    "corner.corner(\n",
    "    analytic_sample, \n",
    "    fig = fig, \n",
    "    color = \"red\", \n",
    "    truths = np.arange(1, 6)**2 * 2, \n",
    "    levels = (0.68, 0.95), \n",
    "    plot_contour=True,\n",
    "    plot_density=False,\n",
    "    plot_datapoints=False,\n",
    ")\n",
    "print(NDE_sample.mean(axis = 0), NDE_sample.std(axis = 0))\n",
    "print(analytic_sample.mean(axis = 0), analytic_sample.std(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recovering posterior\n",
    "To recover posterior we use fitted NDE likelihood and analytic likelihood, together with prior specified at the beginning. For sampling, we use `ultranest` nested sampler.\n",
    "\n",
    "As previously mentioned, for the mock measurement we use 10 i.i.d. samples for $\\mu_0 = 1$ and $\\sigma_0 = 1$. The hope is that final posterior shifts towards those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mock_measurement = np.random.multivariate_normal(np.arange(1, 6)**2, np.diag(np.arange(1, 6)**2), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_gauss(x, mu, sigma):\n",
    "    return -0.5 * np.log(2 * np.pi * sigma) - 0.5 * (x - mu)**2 / sigma**2 \n",
    "\n",
    "def analytic_log_likelihood(d, mu_0, sigma_0):\n",
    "    mu = (np.arange(1, 6)**2).reshape(1, -1) * mu_0.reshape(-1, 1)\n",
    "    cov = (np.arange(1, 6)**2).reshape(1, -1) * sigma_0.reshape(-1, 1)**2\n",
    "    c = -2.5 * np.log(2 * np.pi) - 0.5 * np.log(np.prod(cov, axis = -1))\n",
    "    l = -0.5 * np.sum((d - mu)**2 / cov, axis = -1)\n",
    "    return c + l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ultranest_NDE_posterior(p):\n",
    "    likelihood = np.sum(np.array([NDE.log_prob(mock, p).numpy() for mock in mock_measurement]), axis = 0)\n",
    "    prior = log_gauss(0.0, p[:, 0], 1.0)\n",
    "    return prior + likelihood\n",
    "\n",
    "def ultranest_analytic_posterior(p):\n",
    "    likelihood = np.sum(np.array([analytic_log_likelihood(mock, p[:, 0], p[:, 1]) for mock in mock_measurement]), axis = 0)\n",
    "    prior = log_gauss(0.0, p[:, 0], 1.0)\n",
    "    return prior + likelihood\n",
    "\n",
    "def transformation(p):\n",
    "    x = np.zeros(p.shape, dtype = p.dtype)\n",
    "    x[:, 0] = -5 + 10 * p[:, 0]\n",
    "    x[:, 1] = 0.5 + 4.5 * p[:, 1]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampler_analytic = ultranest.ReactiveNestedSampler(\n",
    "    [\"mu_0\", \"sigma_0\"], \n",
    "    loglike = ultranest_analytic_posterior, \n",
    "    transform = transformation,\n",
    "    vectorized = True,\n",
    "    draw_multiple = True,\n",
    "    ndraw_min = 1000,\n",
    "    ndraw_max = 100000,\n",
    ")\n",
    "result_analytic = sampler_analytic.run(\n",
    "    min_num_live_points = 1000,\n",
    "    min_ess = 1000,\n",
    ")\n",
    "sampler_analytic.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampler_NDE = ultranest.ReactiveNestedSampler(\n",
    "    [\"mu_0\", \"sigma_0\"], \n",
    "    loglike = ultranest_NDE_posterior, \n",
    "    transform = transformation,\n",
    "    vectorized = True,\n",
    "    draw_multiple = True,\n",
    "    ndraw_min = 1000,\n",
    "    ndraw_max = 100000,\n",
    ")\n",
    "result_NDE = sampler_NDE.run(\n",
    "    min_num_live_points = 1000,\n",
    "    min_ess = 1000,\n",
    ")\n",
    "sampler_NDE.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cornerplot(results, fig = None, color = None):\n",
    "    data = np.array(results['weighted_samples']['points'])\n",
    "    weights = np.array(results['weighted_samples']['weights'])\n",
    "    print(data.shape, weights.shape)\n",
    "    cumsumweights = np.cumsum(weights)\n",
    "\n",
    "    mask = cumsumweights > 1e-4\n",
    "\n",
    "    fig = corner.corner(\n",
    "    data[mask, :],\n",
    "    weights = weights[mask], \n",
    "    fig = fig, \n",
    "    color = color, \n",
    "    truths = [1.0, 1.0], \n",
    "    levels = (0.68, 0.95), \n",
    "    plot_contour=True,\n",
    "    plot_density=False,\n",
    "    plot_datapoints=False,\n",
    "    labels = [\"$\\mu_0$\", \"$\\sigma_0$\"]\n",
    ")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5, 5))\n",
    "fig = cornerplot(result_NDE, fig, \"blue\")\n",
    "fig = cornerplot(result_analytic, fig, \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
